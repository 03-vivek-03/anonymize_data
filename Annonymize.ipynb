{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d2b79e4-3221-47da-a793-7434ffb6ccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from paddleocr import PaddleOCR\n",
    "from tqdm.auto import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import shutil\n",
    "import logging\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import pydicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90baef9-f83f-42b9-8109-bc4770e031d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging level for PaddleOCR\n",
    "logging.getLogger(\"ppocr\").setLevel(logging.ERROR)\n",
    "\n",
    "# Initialize PaddleOCR\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang='en')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydicom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4f498f-b56a-4d6e-9802-8b9b0c68edbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize global DataFrame variables\n",
    "done_df = pd.DataFrame(columns=['Folder'])\n",
    "not_done_df = pd.DataFrame(columns=['Folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bce326-bf5f-458f-b864-715d67983ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_dicom_files(root_path, excel_path):\n",
    "    # Check if the master Excel file exists\n",
    "    if os.path.exists(excel_path):\n",
    "        institution_df = pd.read_excel(excel_path)\n",
    "    else:\n",
    "        institution_df = pd.DataFrame(columns=[\"InstitutionName\", \"Counter\"])\n",
    "\n",
    "    dicom_files = []\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.dcm'):\n",
    "                dicom_files.append(os.path.join(root, file))\n",
    "\n",
    "    for dicom_path in tqdm(dicom_files, desc=\"Processing DICOM files\"):\n",
    "        dicom_image = pydicom.dcmread(dicom_path, force=True)\n",
    "        \n",
    "        # Check if PatientName attribute exists\n",
    "        if hasattr(dicom_image, 'PatientName'):\n",
    "            # Extract the patient's age from PatientName\n",
    "            patient_name = str(dicom_image.PatientName)\n",
    "            age_match = re.search(r'(\\d+)(?:Y(?:rs?)?|Year?)$', patient_name, re.IGNORECASE)\n",
    "            if age_match:\n",
    "                dicom_image.PatientAge = age_match.group(1)\n",
    "\n",
    "            # Delete the PatientName, PatientID, and InstitutionName tags\n",
    "            del dicom_image.PatientName\n",
    "            del dicom_image.PatientID\n",
    "            \n",
    "        tag_numbers = [(0x0021, 0x0012), (0x0400, 0x0561), (0x0009, 0x0010), (0x0008, 0x1070), (0x0002, 0x0013), (0x0008, 0x0090), (0x0002, 0x0016), (0x0010, 0x0020)]\n",
    "        \n",
    "        metadata = dicom_image.file_meta\n",
    "        \n",
    "        metadata[(0x0002, 0x0016)].value = ''\n",
    "        metadata[(0x0002, 0x0013)].value = ''\n",
    "\n",
    "        # Handle the InstitutionName tag\n",
    "        if hasattr(dicom_image, 'InstitutionName'):\n",
    "            institution_name = str(dicom_image.InstitutionName)\n",
    "            if institution_name not in institution_df['InstitutionName'].values:\n",
    "                new_counter = len(institution_df) + 1\n",
    "                new_entry = pd.DataFrame({\"InstitutionName\": [institution_name], \"Counter\": [new_counter]})\n",
    "                institution_df = pd.concat([institution_df, new_entry], ignore_index=True)\n",
    "            else:\n",
    "                new_counter = institution_df[institution_df['InstitutionName'] == institution_name]['Counter'].values[0]\n",
    "            dicom_image.InstitutionName = str(new_counter)\n",
    "\n",
    "        for tag_number in tag_numbers:\n",
    "            if tag_number in dicom_image:\n",
    "                del dicom_image[tag_number]\n",
    "\n",
    "        # Save the modified DICOM image with the same name\n",
    "        try:\n",
    "            dicom_image.save_as(dicom_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving DICOM file: {dicom_path}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "\n",
    "    # Save the DataFrame to an Excel file\n",
    "    institution_df.to_excel(excel_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8bf2bb-20e2-46fe-a801-599c69631ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save DataFrames to Excel\n",
    "def save_dataframes():\n",
    "    global done_df, not_done_df, done_file, not_done_file\n",
    "    done_df.to_excel(done_file, index=False)\n",
    "    not_done_df.to_excel(not_done_file, index=False)\n",
    "    print(\"DataFrames saved to Excel.\")\n",
    "\n",
    "def extract_image_tags(soup):\n",
    "    img_tags = soup.find_all('img')\n",
    "    for img_tag in img_tags:\n",
    "        base64str = img_tag['src']\n",
    "        image = base64_to_image(base64str)\n",
    "        if image is None:\n",
    "            continue\n",
    "        image = np.array(image)\n",
    "        \n",
    "        result = ocr.ocr(image, cls=True)\n",
    "        extracted_text = ' '.join([element[1][0] for line in result if line for element in line if element])\n",
    "        \n",
    "        pattern = r\"Reg(.*)\"\n",
    "        match = re.search(pattern, extracted_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            return base64str, match, extracted_text\n",
    "    return None, None, None\n",
    "\n",
    "def base64_to_image(base64str):\n",
    "    base64_string = base64str.split(\",\")[-1]\n",
    "    \n",
    "    # Fix padding issues in base64 string\n",
    "    missing_padding = len(base64_string) % 4\n",
    "    if missing_padding:\n",
    "        base64_string += '=' * (4 - missing_padding)\n",
    "    \n",
    "    try:\n",
    "        image_data = base64.b64decode(base64_string)\n",
    "        image = Image.open(BytesIO(image_data))\n",
    "        return image\n",
    "    except (UnidentifiedImageError, base64.binascii.Error) as e:\n",
    "        logging.warning(f\"Unidentified image file or invalid base64 string: {e}\")\n",
    "        return None\n",
    "\n",
    "def replace_image_with_text(soup, original_base64str, text):\n",
    "    img_tag = None\n",
    "    for tag in soup.find_all('img'):\n",
    "        if original_base64str in tag['src']:\n",
    "            img_tag = tag\n",
    "            break\n",
    "    if img_tag:\n",
    "        img_tag.replace_with(text)\n",
    "\n",
    "def erase_and_save_details(input_folder, excel_path, no_report_path, error_folder):\n",
    "    global done_df, not_done_df, done_file, not_done_file\n",
    "    count = 0\n",
    "    no_report = []\n",
    "\n",
    "    # Initialize an empty DataFrame to store the extracted data\n",
    "    columns = [\"Folder\", \"Patient ID\", \"Patient Name\", \"Age\", \"Gender\", \"Study\", \"Modality\", \"Study Date\", \"Accession\", \"Physician\", \"Extracted Text\", \"Reg No\"]\n",
    "    data_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Load existing done folders from Excel if it exists\n",
    "    done = []\n",
    "    done_file = os.path.join(excel_path, 'done_folders.xlsx')\n",
    "    not_done_file = os.path.join(excel_path, 'not_done_folders.xlsx')\n",
    "    \n",
    "    if os.path.exists(done_file):\n",
    "        done_df = pd.read_excel(done_file)\n",
    "        done = done_df['Folder'].tolist()\n",
    "    \n",
    "    # Ensure directories exist\n",
    "    os.makedirs(no_report_path, exist_ok=True)\n",
    "    os.makedirs(error_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate through each folder in the input folder\n",
    "    for folder in tqdm(os.listdir(input_folder)):\n",
    "        if folder not in done:\n",
    "            inside_folder = os.path.join(input_folder, folder)\n",
    "            html_files = [f for f in os.listdir(inside_folder) if f.endswith('.html')]\n",
    "            \n",
    "            if not html_files:\n",
    "                shutil.move(inside_folder, os.path.join(no_report_path, folder))\n",
    "                no_report.append(folder)\n",
    "            else:\n",
    "                for filename in os.listdir(inside_folder):\n",
    "                    if filename.startswith('Approved'):              \n",
    "                        # Read the HTML file\n",
    "                        with open(os.path.join(inside_folder, filename), 'r', encoding='utf-16') as file:\n",
    "                            html_content = file.read()\n",
    "        \n",
    "                        # Parse the HTML content with BeautifulSoup\n",
    "                        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "                        base64str, match, extracted_text = extract_image_tags(soup)\n",
    "        \n",
    "                        if base64str:    \n",
    "                            patient_name_value = None\n",
    "                            patient_id_value = None\n",
    "                            age_value = None\n",
    "                            sex_value = None\n",
    "                            study_value = None\n",
    "                            modality_value = None\n",
    "                            study_date_value = None\n",
    "                            accession_value = None\n",
    "                            physician_value = None\n",
    "        \n",
    "                            # Find and erase 'Patient Name' and 'Patient ID' values\n",
    "                            for tag in soup.find_all('td'):\n",
    "                                if tag.find('b') and 'Patient Name' in tag.find('b').text:\n",
    "                                    patient_name_value = tag.get_text().strip().replace('Patient Name:', '').strip()\n",
    "                                    age_match = re.search(r'\\d+', patient_name_value)\n",
    "                                    if age_match:\n",
    "                                        age_value = patient_name_value[age_match.start():].strip()\n",
    "                                        # Remove the age part from patient_name_value\n",
    "                                        patient_name_value = patient_name_value[:age_match.start()].strip()\n",
    "                                    # Replace text after 'Patient Name' with empty string\n",
    "                                    tag.contents[-1].replace_with('')\n",
    "        \n",
    "                                if tag.find('b') and 'Patient ID' in tag.find('b').text:\n",
    "                                    patient_id_value = tag.get_text().strip().replace('Patient ID:', '').strip()\n",
    "                                    tag.contents[-1].replace_with('')\n",
    "        \n",
    "                                if tag.find('b') and 'Sex' in tag.find('b').text:\n",
    "                                    sex_value = tag.get_text().strip().replace('Sex:', '').strip()\n",
    "        \n",
    "                                if tag.find('b') and 'Modality' in tag.find('b').text:\n",
    "                                    modality_value = tag.get_text().strip().replace('Modality:', '').strip()\n",
    "        \n",
    "                                if tag.find('b') and 'Study' in tag.find('b').text and not 'Study ' in tag.find('b').text:\n",
    "                                    study_value = tag.get_text().strip().replace('Study:', '').strip()\n",
    "        \n",
    "                                if tag.find('b') and 'Study Date' in tag.find('b').text:\n",
    "                                    study_date_value = tag.get_text().strip().replace('Study Date:', '').strip()\n",
    "        \n",
    "                                if tag.find('b') and 'Accession Number' in tag.find('b').text:\n",
    "                                    accession_value = tag.get_text().strip().replace('Accession Number:', '').strip()\n",
    "                                    tag.contents[-1].replace_with('')\n",
    "        \n",
    "                                if tag.find('b') and 'Referring Physician' in tag.find('b').text:\n",
    "                                    physician_value = tag.get_text().strip().replace('Referring Physician:', '').strip()\n",
    "                                    tag.contents[-1].replace_with('')\n",
    "        \n",
    "                            # Insert the age value into the <b>Age</b> tag if it exists\n",
    "                            if age_value:\n",
    "                                for tag in soup.find_all('td'):\n",
    "                                    if tag.find('b') and 'Age' in tag.find('b').text:\n",
    "                                        # Replace ':' with ': ' + age_value\n",
    "                                        tag.contents[-1].replace_with(f':{age_value}')\n",
    "                            else:\n",
    "                                for tag in soup.find_all('td'):\n",
    "                                    if tag.find('b') and 'Age' in tag.find('b').text:\n",
    "                                        age_value = tag.get_text().strip().replace('Age:', '').strip()\n",
    "                                        if not age_value:\n",
    "                                            age_value = \"0\"\n",
    "        \n",
    "                            if match:\n",
    "                                reg_no = match.group(0).strip()\n",
    "                                replace_image_with_text(soup, base64str, reg_no)\n",
    "        \n",
    "                                # Create a DataFrame row with the extracted data\n",
    "                                data_row = {\n",
    "                                    \"Folder\": folder,\n",
    "                                    \"Patient ID\": patient_id_value,\n",
    "                                    \"Patient Name\": patient_name_value,\n",
    "                                    \"Age\": age_value,\n",
    "                                    \"Gender\": sex_value,\n",
    "                                    \"Study\": study_value,\n",
    "                                    \"Modality\": modality_value,\n",
    "                                    \"Study Date\": study_date_value,\n",
    "                                    \"Accession\": accession_value,\n",
    "                                    \"Physician\": physician_value,\n",
    "                                    \"Extracted Text\": extracted_text,\n",
    "                                    \"Reg No\": reg_no\n",
    "                                }\n",
    "                                \n",
    "                                # Append the data row to the DataFrame\n",
    "                                data_df = pd.concat([data_df, pd.DataFrame([data_row])], ignore_index=True)\n",
    "            \n",
    "                                # Write the modified HTML to the output folder\n",
    "                                with open(os.path.join(inside_folder, filename), 'w', encoding='utf-16') as file:\n",
    "                                    file.write(str(soup))\n",
    "    \n",
    "                                done_df = pd.concat([done_df, pd.DataFrame([{'Folder': folder}])], ignore_index=True)\n",
    "                            else:\n",
    "                                count += 1\n",
    "                                not_done_df = pd.concat([not_done_df, pd.DataFrame([{'Folder': folder}])], ignore_index=True)\n",
    "                                shutil.move(inside_folder, os.path.join(error_folder, folder))\n",
    "                                break\n",
    "                        else:\n",
    "                            count += 1\n",
    "                            not_done_df = pd.concat([done_df, pd.DataFrame([{'Folder': folder}])], ignore_index=True)\n",
    "                            shutil.move(inside_folder, os.path.join(error_folder, folder))\n",
    "    \n",
    "    # Save the data DataFrame to an Excel file\n",
    "    data_df.to_excel(os.path.join(excel_path, 'extracted_data.xlsx'), index=False)\n",
    "    \n",
    "    # Save the done folders DataFrame to an Excel file\n",
    "    done_df.to_excel(done_file, index=False)\n",
    "    \n",
    "    # Save the not done folders DataFrame to an Excel file\n",
    "    not_done_df.to_excel(not_done_file, index=False)\n",
    "    \n",
    "    # Print the count after the main loop\n",
    "    print(f\"Total count of folders moved to error folder: {count}\")\n",
    "    print(f\"Total count of folders without report: {len(no_report)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe20d279-c6ee-41cd-9a5c-380745c6d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = r\"C:\\Users\\Techjiva\\Downloads\\Sagar\\WRIST FRACTURE\"\n",
    "error_folder = r\"C:\\Users\\Techjiva\\Downloads\\Sagar\"\n",
    "no_report_path = r\"C:\\Users\\Techjiva\\Downloads\\Sagar\"\n",
    "excel_path = r\"C:\\Users\\Techjiva\\Downloads\\Sagar\\WRIST FRACTURE\"\n",
    "dic_excel_path = \"C:/Users/Techjiva/Downloads/Sagar/WRIST FRACTURE/Institution_names.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fa9ec6-9ca2-4871-aaa1-3fbb89bbc759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the main function\n",
    "erase_and_save_details(input_folder, excel_path, no_report_path, error_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e6686-0e9b-48c6-bf2a-5b4c2a590a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "modify_dicom_files(input_folder, dic_excel_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
